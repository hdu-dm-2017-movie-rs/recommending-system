{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of the ratings data frame:', (26024289, 4))\n",
      "('Shape of the tags data frame:', (753170, 4))\n",
      "('Shape of the movies data frame:', (45843, 3))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# 加载数据集\n",
    "ratings = pd.read_csv('ml-latest/ratings.csv')\n",
    "print ('Shape of the ratings data frame:', ratings.shape)\n",
    "\n",
    "tags = pd.read_csv('ml-latest/tags.csv')\n",
    "print ('Shape of the tags data frame:', tags.shape)\n",
    "\n",
    "movies = pd.read_csv('ml-latest/movies.csv')\n",
    "print ('Shape of the movies data frame:', movies.shape)\n",
    "\n",
    "tags = tags.sample(frac=0.2)\n",
    "ratings = ratings.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of the joint data frame:', (26193, 5))\n",
      "   userId  movieId  rating              tag   timestamp\n",
      "0  256510     5869     2.5       Bibliothek  1138659665\n",
      "1  168200    69122     4.0           comedy  1292434606\n",
      "2  172179    37733     1.5    disappointing  1264106059\n",
      "3  190554    63072     4.5  Viggo Mortensen  1263758929\n",
      "4  169568     3362     4.0        Al Pacino  1255086480\n"
     ]
    }
   ],
   "source": [
    "#除去时间，结合ratings和tags的数据\n",
    "ratings = ratings.drop(['timestamp'],axis=1)\n",
    "#ratings.head(n=5)\n",
    "#Display summary statistics about data\n",
    "#ratings.describe()\n",
    "#Print sample tags data\n",
    "#tags.head(n=5)\n",
    "#Print sample movies data\n",
    "#movies.head(n=5)\n",
    "data = pd.merge(ratings, tags, how='inner')\n",
    "print ('Shape of the joint data frame:', data.shape)\n",
    "print (data.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理 检查数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('userId', 4054)\n",
      "('movieId', 5487)\n",
      "('rating', 10)\n",
      "('tag', 7416)\n",
      "('timestamp', 25772)\n"
     ]
    }
   ],
   "source": [
    "#提取每个标签的数据个数\n",
    "for column in data.columns:\n",
    "    print (column, data[column].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2006-01-31 06:21:05\n",
       "1   2010-12-16 01:36:46\n",
       "2   2010-01-22 04:34:19\n",
       "3   2010-01-18 04:08:49\n",
       "4   2009-10-09 19:08:00\n",
       "5   2016-10-13 09:10:04\n",
       "6   2016-10-13 09:07:06\n",
       "7   2015-09-27 17:17:26\n",
       "8   2017-01-04 02:32:53\n",
       "9   2017-01-04 02:32:59\n",
       "Name: timestamp, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从tag中删除任何特殊字符，以减少惟一标记的数量，并提高性能\n",
    "data['rating'] = data['rating'].apply(lambda x: 1 if x > 4 else 0)\n",
    "data['tag'] = data['tag'].apply(lambda x: str(x))\n",
    "data['tag'] = data['tag'].map(lambda x: re.sub(r'([^\\s\\w]|_)+', '', x))\n",
    "data['tag'] = data['tag'].str.lower()\n",
    "#将时间转换为datetime格式\n",
    "data['timestamp'] = data['timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "data['timestamp'].astype('datetime64[ns]')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6575 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['tag'])\n",
    "sequences = tokenizer.texts_to_sequences(data['tag'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "pseq = pad_sequences(sequences)\n",
    "pdseq = pd.DataFrame(pseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of words in plot summary: ', 6391)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english',decode_error='ignore', analyzer='word')\n",
    "corpus = data['tag'].values\n",
    "wordvec = vectorizer.fit_transform(corpus.ravel())\n",
    "wordvec = wordvec.toarray()\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "print(\"number of words in plot summary: \", len(words))\n",
    "pdwordvec = pd.DataFrame(wordvec,columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备深度学习的数据\n",
    "将斯坦福的glove.6B词汇嵌入作为预先训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(words), 100))\n",
    "for i in range(len(words)):\n",
    "    embedding_vector = embeddings_index.get(words[i])\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "pdembedding = pd.DataFrame(embedding_matrix.T,columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM的深度学习只使用单词序列\n",
    "dpdata = pd.concat([data, pdseq], axis=1)\n",
    "dpdata = dpdata.drop(['tag'], axis=1)\n",
    "dpdata = dpdata.drop(['userId'], axis=1)\n",
    "dpdata = dpdata.drop(['movieId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training data set: 20954\n",
      "Number of rows in test data set: 5239\n"
     ]
    }
   ],
   "source": [
    "#构建训练和数据集\n",
    "#train = dpdata[(dpdata['timestamp'] < '2016-08-01') ]\n",
    "#test = dpdata[(dpdata['timestamp'] >= '2016-08-01') ]\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(dpdata, test_size=0.2, random_state=0)\n",
    "print \"Number of rows in training data set:\",(len(train))\n",
    "print \"Number of rows in test data set:\", (len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除时间戳\n",
    "train = train.drop(['timestamp'], axis=1)\n",
    "test = test.drop(['timestamp'], axis=1)\n",
    "y_train = train['rating']\n",
    "y_test = test['rating']\n",
    "x_train = train.drop(['rating'], axis=1)\n",
    "x_test = test.drop(['rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, None, 100)         657600    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 10)                4440      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 662,062\n",
      "Trainable params: 4,462\n",
      "Non-trainable params: 657,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "y_test_matrix = to_categorical(y_test)\n",
    "y_train_matrix = to_categorical(y_train)\n",
    "x_train_array = np.array(x_train)\n",
    "x_test_array = np.array(x_test)\n",
    "epochs = 20\n",
    "lrate = 0.01\n",
    "sgd = SGD(lr=lrate)\n",
    "early_stopping = EarlyStopping(monitor='acc',patience=2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 100, mask_zero=True, trainable=False))\n",
    "model.add(LSTM(10, return_sequences=False))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20954 samples, validate on 5239 samples\n",
      "Epoch 1/20\n",
      "20954/20954 [==============================] - 11s 519us/step - loss: 0.6484 - acc: 0.6273 - val_loss: 0.6591 - val_acc: 0.6217\n",
      "Epoch 2/20\n",
      "20954/20954 [==============================] - 8s 400us/step - loss: 0.6480 - acc: 0.6278 - val_loss: 0.6591 - val_acc: 0.6207\n",
      "Epoch 3/20\n",
      "20954/20954 [==============================] - 7s 346us/step - loss: 0.6477 - acc: 0.6281 - val_loss: 0.6590 - val_acc: 0.6221\n",
      "Epoch 4/20\n",
      "20954/20954 [==============================] - 7s 350us/step - loss: 0.6476 - acc: 0.6279 - val_loss: 0.6587 - val_acc: 0.6230\n",
      "Epoch 5/20\n",
      "20954/20954 [==============================] - 11s 520us/step - loss: 0.6473 - acc: 0.6278 - val_loss: 0.6594 - val_acc: 0.6217\n",
      "Epoch 6/20\n",
      "20954/20954 [==============================] - 10s 476us/step - loss: 0.6474 - acc: 0.6279 - val_loss: 0.6591 - val_acc: 0.6221\n",
      "Epoch 7/20\n",
      "20954/20954 [==============================] - 7s 347us/step - loss: 0.6466 - acc: 0.6279 - val_loss: 0.6615 - val_acc: 0.6223\n",
      "Epoch 8/20\n",
      "20954/20954 [==============================] - 7s 348us/step - loss: 0.6467 - acc: 0.6284 - val_loss: 0.6593 - val_acc: 0.6213\n",
      "Epoch 9/20\n",
      "20954/20954 [==============================] - 11s 526us/step - loss: 0.6463 - acc: 0.6275 - val_loss: 0.6595 - val_acc: 0.6217\n",
      "Epoch 10/20\n",
      "20954/20954 [==============================] - 8s 392us/step - loss: 0.6458 - acc: 0.6272 - val_loss: 0.6591 - val_acc: 0.6213\n",
      "Epoch 11/20\n",
      "20954/20954 [==============================] - 7s 355us/step - loss: 0.6458 - acc: 0.6297 - val_loss: 0.6601 - val_acc: 0.6221\n",
      "Epoch 12/20\n",
      "20954/20954 [==============================] - 8s 362us/step - loss: 0.6450 - acc: 0.6289 - val_loss: 0.6599 - val_acc: 0.6226\n",
      "Epoch 13/20\n",
      "20954/20954 [==============================] - 10s 492us/step - loss: 0.6453 - acc: 0.6297 - val_loss: 0.6599 - val_acc: 0.6228\n",
      "Epoch 14/20\n",
      "20954/20954 [==============================] - 8s 367us/step - loss: 0.6445 - acc: 0.6300 - val_loss: 0.6597 - val_acc: 0.6223\n",
      "Epoch 15/20\n",
      "20954/20954 [==============================] - 7s 350us/step - loss: 0.6447 - acc: 0.6288 - val_loss: 0.6602 - val_acc: 0.6226\n",
      "Epoch 16/20\n",
      "20954/20954 [==============================] - 8s 375us/step - loss: 0.6438 - acc: 0.6290 - val_loss: 0.6596 - val_acc: 0.6234\n",
      "Epoch 17/20\n",
      "20954/20954 [==============================] - 8s 386us/step - loss: 0.6435 - acc: 0.6309 - val_loss: 0.6599 - val_acc: 0.6221\n",
      "Epoch 18/20\n",
      "20954/20954 [==============================] - 8s 392us/step - loss: 0.6433 - acc: 0.6301 - val_loss: 0.6598 - val_acc: 0.6245\n",
      "Epoch 19/20\n",
      "20954/20954 [==============================] - 8s 380us/step - loss: 0.6428 - acc: 0.6314 - val_loss: 0.6597 - val_acc: 0.6232\n",
      "Epoch 20/20\n",
      "20954/20954 [==============================] - 8s 369us/step - loss: 0.6426 - acc: 0.6314 - val_loss: 0.6608 - val_acc: 0.6236\n",
      "Accuracy: 62.3592%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model \n",
    "#model.fit(x_train_array, y_train_matrix, validation_data=(x_test_array, y_ test_matrix), epochs=epochs, batch_size=100, verbose=1, class_weight='balanced')\n",
    "model.fit(x_train_array, y_train_matrix, validation_data=(x_test_array, y_test_matrix), epochs=epochs, batch_size=50, class_weight='balanced')\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test_array, y_test_matrix, verbose=0)\n",
    "print(\"Accuracy: %.4f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, None, 100)         657600    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 10)                3330      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 660,952\n",
      "Trainable params: 3,352\n",
      "Non-trainable params: 657,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 100, mask_zero=True, trainable=False))\n",
    "model.add(GRU(10, return_sequences=False))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20954 samples, validate on 5239 samples\n",
      "Epoch 1/20\n",
      "20954/20954 [==============================] - 6s 278us/step - loss: 0.6584 - acc: 0.6184 - val_loss: 0.6585 - val_acc: 0.6263\n",
      "Epoch 2/20\n",
      "20954/20954 [==============================] - 6s 292us/step - loss: 0.6579 - acc: 0.6184 - val_loss: 0.6583 - val_acc: 0.6255\n",
      "Epoch 3/20\n",
      "20954/20954 [==============================] - 6s 304us/step - loss: 0.6574 - acc: 0.6186 - val_loss: 0.6582 - val_acc: 0.6268\n",
      "Epoch 4/20\n",
      "20954/20954 [==============================] - 9s 415us/step - loss: 0.6566 - acc: 0.6191 - val_loss: 0.6580 - val_acc: 0.6249\n",
      "Epoch 5/20\n",
      "20954/20954 [==============================] - 6s 285us/step - loss: 0.6560 - acc: 0.6196 - val_loss: 0.6575 - val_acc: 0.6272\n",
      "Epoch 6/20\n",
      "20954/20954 [==============================] - 6s 285us/step - loss: 0.6553 - acc: 0.6201 - val_loss: 0.6572 - val_acc: 0.6284\n",
      "Epoch 7/20\n",
      "20954/20954 [==============================] - 6s 295us/step - loss: 0.6548 - acc: 0.6205 - val_loss: 0.6573 - val_acc: 0.6278\n",
      "Epoch 8/20\n",
      "20954/20954 [==============================] - 8s 378us/step - loss: 0.6542 - acc: 0.6204 - val_loss: 0.6572 - val_acc: 0.6280\n",
      "Epoch 9/20\n",
      "20954/20954 [==============================] - 6s 308us/step - loss: 0.6536 - acc: 0.6205 - val_loss: 0.6569 - val_acc: 0.6282\n",
      "Epoch 10/20\n",
      "20954/20954 [==============================] - 6s 306us/step - loss: 0.6532 - acc: 0.6205 - val_loss: 0.6576 - val_acc: 0.6272\n",
      "Epoch 11/20\n",
      "20954/20954 [==============================] - 6s 306us/step - loss: 0.6528 - acc: 0.6206 - val_loss: 0.6568 - val_acc: 0.6266\n",
      "Epoch 12/20\n",
      "20954/20954 [==============================] - 6s 306us/step - loss: 0.6522 - acc: 0.6213 - val_loss: 0.6568 - val_acc: 0.6259\n",
      "Epoch 13/20\n",
      "20954/20954 [==============================] - 6s 302us/step - loss: 0.6518 - acc: 0.6222 - val_loss: 0.6573 - val_acc: 0.6242\n",
      "Epoch 14/20\n",
      "20954/20954 [==============================] - 6s 294us/step - loss: 0.6515 - acc: 0.6225 - val_loss: 0.6587 - val_acc: 0.6205\n",
      "Epoch 15/20\n",
      "20954/20954 [==============================] - 6s 300us/step - loss: 0.6513 - acc: 0.6232 - val_loss: 0.6573 - val_acc: 0.6263\n",
      "Epoch 16/20\n",
      "20954/20954 [==============================] - 7s 355us/step - loss: 0.6510 - acc: 0.6235 - val_loss: 0.6569 - val_acc: 0.6228\n",
      "Epoch 17/20\n",
      "20954/20954 [==============================] - 8s 405us/step - loss: 0.6506 - acc: 0.6218 - val_loss: 0.6586 - val_acc: 0.6276\n",
      "Epoch 18/20\n",
      "20954/20954 [==============================] - 7s 352us/step - loss: 0.6503 - acc: 0.6230 - val_loss: 0.6577 - val_acc: 0.6259\n",
      "Epoch 19/20\n",
      "20954/20954 [==============================] - 7s 324us/step - loss: 0.6500 - acc: 0.6227 - val_loss: 0.6590 - val_acc: 0.6211\n",
      "Epoch 20/20\n",
      "20954/20954 [==============================] - 7s 322us/step - loss: 0.6499 - acc: 0.6239 - val_loss: 0.6587 - val_acc: 0.6249\n",
      "Accuracy: 62.4928%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_array, y_train_matrix, validation_data=(x_test_array, y_test_matrix), epochs=epochs, batch_size=50, class_weight='balanced')\n",
    "scores = model.evaluate(x_test_array, y_test_matrix, verbose=0)\n",
    "print(\"Accuracy: %.4f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 和传统方法进行加权计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the test set for random forest is:  0.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFC = RandomForestClassifier(class_weight='balanced')\n",
    "RFC.set_params(n_estimators=100)\n",
    "RFC.fit(x_train,y_train)\n",
    "y_pred = RFC.predict_proba(x_test)\n",
    "R2_rfc = RFC.score(x_test,y_test) \n",
    "print \"Accuracy of the test set for random forest is: \", np.round(R2_rfc,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5239/5239 [==============================] - 1s 103us/step\n",
      "Accuracy of the test set for Ensemble model is:  0.63\n"
     ]
    }
   ],
   "source": [
    "#Calculate weighted probabilities \n",
    "y_predlstm = model.predict_proba(x_test_array)\n",
    "y_pre = (0.6*y_predlstm + 0.4*y_pred)\n",
    "\n",
    "#Predict ratings using the weighted probabilities\n",
    "y_predensem = np.zeros((len(y_pre)))\n",
    "for i in range(len(y_pre)):\n",
    "    if y_pre[i,1] >= 0.5:\n",
    "        y_predensem[i] = 1\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print \"Accuracy of the test set for Ensemble model is: \", np.round(accuracy_score(y_test, y_predensem),2)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
